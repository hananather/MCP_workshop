{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdc308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effa0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR = \"papers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a6d0f",
   "metadata": {},
   "source": [
    "\n",
    "This function searches arXiv for academic papers and saves their metadata (NOT the full paper content). We create a new arXiv client each time the function runs. It's not a persistent connection  just an API wrapper that makes HTTP requests (Think of it like opening a web browser, doing a search, then closing it). \n",
    "In summary: \n",
    "\n",
    "- Takes a topic as input (e.g., \"machine learning\", \"quantum computing\")\n",
    "- Creates a dedicated folder for each topic: `PAPER_DIR/topic_name/`\n",
    "- Searches arXiv and saves paper metadata to `papers_info.json` in that folder\n",
    "- Returns a list of paper IDs found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4679354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    Search for papers on arXiv based on a topic and store their information.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic to search for\n",
    "        max_results: Maximum number of results to retrieve (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of paper IDs found in the search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use arxiv to find the papers \n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # Search for the most relevant articles matching the queried topic\n",
    "    search = arxiv.Search(\n",
    "        query = topic,\n",
    "        max_results = max_results,\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = client.results(search)\n",
    "    \n",
    "    # Create directory for this topic\n",
    "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(path, \"papers_info.json\")\n",
    "\n",
    "    # Try to load existing papers info\n",
    "    try:\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            papers_info = json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info = {}\n",
    "\n",
    "    # Process each paper and add to papers_info  \n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id())\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date())\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "    \n",
    "    # Save updated papers_info to json file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(papers_info, json_file, indent=2)\n",
    "    \n",
    "    print(f\"Results are saved in: {file_path}\")\n",
    "    \n",
    "    return paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "id": "5986d8d3-0af7-4479-93e2-2672a643c662",
   "metadata": {},
   "outputs": [],
   "source": "search_papers(\"Neural Networks\")"
  },
  {
   "cell_type": "code",
   "id": "ead67895-bfca-4820-bfd1-a95f17959a2d",
   "metadata": {},
   "outputs": [],
   "source": "def extract_info(paper_id: str) -> str:\n    \"\"\"\n    Search for information about a specific paper across all topic directories.\n    \n    Args:\n        paper_id: The ID of the paper to look for\n        \n    Returns:\n        JSON string with paper information if found, error message if not found\n    \"\"\"\n \n    for topic_dir in os.listdir(PAPER_DIR):\n        topic_path = os.path.join(PAPER_DIR, topic_dir)\n        if os.path.isdir(topic_path):\n            file_path = os.path.join(topic_path, \"papers_info.json\")\n            if os.path.isfile(file_path):\n                try:\n                    with open(file_path, \"r\") as json_file:\n                        papers_info = json.load(json_file)\n                        if paper_id in papers_info:\n                            return json.dumps(papers_info[paper_id], indent=2)\n                except (FileNotFoundError, json.JSONDecodeError) as e:\n                    print(f\"Error reading {file_path}: {str(e)}\")\n                    continue\n    \n    return f\"There's no saved information related to paper {paper_id}.\""
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c320bb42-34dc-4adc-8365-da3a9af9ea69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"title\": \"Median Binary-Connect Method and a Binary Convolutional Neural Nework for Word Recognition\",\\n  \"authors\": [\\n    \"Spencer Sheen\",\\n    \"Jiancheng Lyu\"\\n  ],\\n  \"summary\": \"We propose and study a new projection formula for training binary weight\\\\nconvolutional neural networks. The projection formula measures the error in\\\\napproximating a full precision (32 bit) vector by a 1-bit vector in the l_1\\\\nnorm instead of the standard l_2 norm. The l_1 projector is in closed\\\\nanalytical form and involves a median computation instead of an arithmatic\\\\naverage in the l_2 projector. Experiments on 10 keywords classification show\\\\nthat the l_1 (median) BinaryConnect (BC) method outperforms the regular BC,\\\\nregardless of cold or warm start. The binary network trained by median BC and a\\\\nrecent blending technique reaches test accuracy 92.4%, which is 1.1% lower than\\\\nthe full-precision network accuracy 93.5%. On Android phone app, the trained\\\\nbinary network doubles the speed of full-precision network in spoken keywords\\\\nrecognition.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/1811.02784v1\",\\n  \"published\": \"2018-11-07\"\\n}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info('1811.02784v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f6585e-bff2-473e-ae77-a8d248c02a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"name\": \"search_papers\",\n",
    "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic to search for\"\n",
    "                }, \n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results to retrieve\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"extract_info\",\n",
    "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paper_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ID of the paper to look for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"paper_id\"]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdb25bfb",
   "metadata": {},
   "outputs": [],
   "source": "mapping_tool_function = {\n    \"search_papers\": search_papers,\n    \"extract_info\": extract_info\n}\n\ndef execute_tool(tool_name, tool_args):\n    \n    result = mapping_tool_function[tool_name](**tool_args)\n\n    if result is None:\n        result = \"The operation completed but didn't return any results.\"\n        \n    elif isinstance(result, list):\n        result = ', '.join(result)\n        \n    elif isinstance(result, dict):\n        # Convert dictionaries to formatted JSON strings\n        result = json.dumps(result, indent=2)\n    \n    else:\n        # For any other type, convert using str()\n        result = str(result)\n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43aebe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a5ef118",
   "metadata": {},
   "outputs": [],
   "source": "def process_query(query):\n    \n    messages = [{'role': 'user', 'content': query}]\n    \n    response = client.messages.create(max_tokens = 2024,\n                                  model = 'claude-3-7-sonnet-20250219', \n                                  tools = tools,\n                                  messages = messages)\n    \n    continue_processing = True\n    while continue_processing:\n        assistant_content = []\n\n        for content in response.content:\n            if content.type == 'text':\n                \n                print(content.text)\n                assistant_content.append(content)\n                \n                if len(response.content) == 1:\n                    continue_processing = False\n            \n            elif content.type == 'tool_use':\n                \n                assistant_content.append(content)\n                messages.append({'role': 'assistant', 'content': assistant_content})\n                \n                tool_id = content.id\n                tool_args = content.input\n                tool_name = content.name\n                print(f\"Calling tool {tool_name} with args {tool_args}\")\n                \n                result = execute_tool(tool_name, tool_args)\n                messages.append({\"role\": \"user\", \n                                  \"content\": [\n                                      {\n                                          \"type\": \"tool_result\",\n                                          \"tool_use_id\": tool_id,\n                                          \"content\": result\n                                      }\n                                  ]\n                                })\n                response = client.messages.create(max_tokens = 2024,\n                                  model = 'claude-3-7-sonnet-20250219', \n                                  tools = tools,\n                                  messages = messages) \n                \n                if len(response.content) == 1 and response.content[0].type == \"text\":\n                    print(response.content[0].text)\n                    continue_processing = False"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf82cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == 'quit':\n",
    "                break\n",
    "    \n",
    "            process_query(query)\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f865890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff9f14",
   "metadata": {},
   "source": "## arXiv Paper Search System: Technical Implementation\n\nThis system implements a function-calling agent that interfaces with arXiv's API to search and retrieve academic papers. The architecture demonstrates fundamental patterns in LLM tool integration, specifically how to bridge Python functions with language model capabilities through the Model Context Protocol (MCP).\n\n**Core Design Decision**: Store papers as JSON files in a hierarchical directory structure rather than a database. This choice optimizes for simplicity and human readability - each topic gets its own directory, making manual inspection straightforward. The trade-off is query performance at scale, but for typical research workflows this is acceptable.\n\n```python\npapers/\n├── neural_networks/\n│   └── papers_info.json\n├── machine_learning/\n│   └── papers_info.json\n└── quantum_computing/\n    └── papers_info.json\n```\n\nThe `search_papers` function creates this structure dynamically. When searching for \"Neural Networks\", it normalizes the topic to `neural_networks` (lowercase, underscores for spaces) to ensure valid directory names. The function is idempotent - multiple calls with the same topic append new papers to the existing JSON file rather than overwriting.\n\n```python\ndef search_papers(topic: str, max_results: int = 5) -> List[str]:\n    client = arxiv.Client()  # Not a persistent connection - just an API wrapper\n    \n    # Normalize topic for filesystem: \"Neural Networks\" → \"neural_networks\"\n    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n    os.makedirs(path, exist_ok=True)  # exist_ok prevents FileExistsError on repeated calls\n```\n\nThe arxiv.Client() instantiation is lightweight - it's not maintaining a connection but rather creating an object that knows how to make HTTP requests to arXiv's API. Think of it as initializing a requests session with arXiv-specific configuration.\n\n**Error Handling Pattern**: The function implements defensive programming by attempting to load existing data, falling back to an empty dictionary if the file doesn't exist or contains invalid JSON:\n\n```python\ntry:\n    with open(file_path, \"r\") as json_file:\n        papers_info = json.load(json_file)\nexcept (FileNotFoundError, json.JSONDecodeError):\n    papers_info = {}\n```\n\nThis pattern ensures the function never crashes due to missing or corrupted data files. The append-only design means we accumulate papers over time - useful for building a personal research database.\n\nThe `extract_info` function implements a linear search across all topic directories. While O(n) in the number of topics, this is acceptable because:\n1. Topics are limited (typically dozens, not thousands)\n2. File I/O dominates performance anyway\n3. The alternative (maintaining a global index) adds complexity\n\n```python\ndef extract_info(paper_id: str) -> str:\n    for topic_dir in os.listdir(PAPER_DIR):  # Better name than 'item'\n        topic_path = os.path.join(PAPER_DIR, topic_dir)\n        if os.path.isdir(topic_path):\n            json_file_path = os.path.join(topic_path, \"papers_info.json\")\n            # ... load and search\n```\n\n**Function Registry Pattern**: The `mapping_tool_function` dictionary implements dynamic dispatch - a common pattern in plugin architectures and RPC systems:\n\n```python\nmapping_tool_function = {\n    \"search_papers\": search_papers,\n    \"extract_info\": extract_info\n}\n\ndef execute_tool(tool_name, tool_args):\n    result = mapping_tool_function[tool_name](**tool_args)\n```\n\nThis design enables adding new tools without modifying the dispatch logic. The `**tool_args` syntax unpacks the dictionary into keyword arguments, so `{\"topic\": \"AI\", \"max_results\": 10}` becomes `search_papers(topic=\"AI\", max_results=10)`.\n\n**String Conversion for LLM Compatibility**: Language models operate on text, not Python objects. The execute_tool function normalizes all return types to strings:\n\n```python\nif isinstance(result, list):\n    result = ', '.join(result)  # ['id1', 'id2'] → \"id1, id2\"\nelif isinstance(result, dict):\n    result = json.dumps(result, indent=2)  # Pretty-printed JSON\n```\n\nThis ensures the LLM receives parseable text regardless of what the underlying function returns.\n\n**The Agent Loop**: The `process_query` function implements the core agent pattern - a while loop that alternates between LLM reasoning and tool execution:\n\n```python\ndef process_query(query):\n    messages = [{'role': 'user', 'content': query}]\n    response = client.messages.create(model='claude-3-5-sonnet-20241022', \n                                    tools=tools, messages=messages)\n    \n    continue_processing = True  # Better name than shadowing function name\n    while continue_processing:\n        for content in response.content:\n            if content.type == 'text':\n                print(content.text)\n                if len(response.content) == 1:\n                    continue_processing = False\n            \n            elif content.type == 'tool_use':\n                # Execute tool and append result to conversation\n                result = execute_tool(content.name, content.input)\n                messages.append({\"role\": \"user\", \n                                \"content\": [{\"type\": \"tool_result\",\n                                           \"tool_use_id\": content.id,\n                                           \"content\": result}]})\n                # Get next response from model\n                response = client.messages.create(...)\n```\n\nThis implements the fundamental insight: **agents are while loops with function calls**. The loop continues until the model produces a text-only response (no tool calls), indicating it has completed the task.\n\n**Message Protocol**: The conversation follows a specific format for tool results:\n\n```python\n{\n    \"role\": \"user\",\n    \"content\": [{\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"unique-id-from-model\",\n        \"content\": \"string-result-from-tool\"\n    }]\n}\n```\n\nThis format tells the model \"here's what your tool call returned, continue processing\". The model maintains conversation state through the messages array - each append adds to the context.\n\n**Error Propagation**: The system implements graceful degradation at each layer:\n- File operations: Continue searching other directories if one fails\n- Tool execution: Catch exceptions and return error messages\n- Chat loop: Display errors without crashing the interface\n\n```python\ndef chat_loop():\n    while True:\n        try:\n            query = input(\"\\nQuery: \").strip()\n            if query.lower() == 'quit':\n                break\n            process_query(query)\n        except Exception as e:\n            print(f\"\\nError: {str(e)}\")  # Show error but continue\n```\n\n**Performance Considerations**: \n- JSON files are loaded entirely into memory - fine for hundreds of papers per topic\n- No caching between queries - each search reloads files\n- Synchronous I/O throughout - could benefit from async for multiple searches\n\n**Security Notes**: The system trusts user input for file paths (via topic names) which could enable directory traversal attacks in a production environment. The `.lower().replace(\" \", \"_\")` normalization provides some protection but isn't comprehensive.\n\nThis implementation demonstrates core patterns in LLM tool integration: function registries for dynamic dispatch, string normalization for model compatibility, conversation state management through message arrays, and the fundamental agent loop. These patterns appear across frameworks like LangChain, LlamaIndex, and Pydantic AI - understanding this implementation provides insight into how those abstractions work internally.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}